{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macy Product List Crawling\n",
    "\n",
    "The task is to get a list of the available products on the macys.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: collect paths\n",
    "\n",
    "The most straightforward idea is to go to the sitemap of macys.com\n",
    "\n",
    "The sitemap looks like:\n",
    "* category 1:\n",
    "    * sub-category 1\n",
    "    * sub-category 2\n",
    "    * ...\n",
    "* category 2:\n",
    "    * ...\n",
    "\n",
    "Then we can collect the urls of each category, and crawling all the products in each category and subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.macys.com/cms/slp/2/Site-Index'\n",
    "## you may need to change your agent based on your browser\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'} \n",
    "response = requests.get(url, headers=headers)\n",
    "soup =BeautifulSoup(response.text, 'html.parser') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by investigating the html of sitemap, it is found that all the urls are embedded in the div block with class sitelink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "divTags = soup.find_all(\"div\", {\"class\": \"sitelink_container\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By checking the categories:\n",
    "\n",
    "* some sub-category is the same as their parent like Pants have subcategory pants\n",
    "* some sub-category contains the \"ALL\" \n",
    "\n",
    "for these situation, the other subcategory in the same category don't have to be included to avoid repeated searching\n",
    "\n",
    "For the categories which are not products like store locations, they don't need to be included\n",
    "\n",
    "Categories of Bath, Shoes, Watches, Juniors that don't go to list are edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_list=[]\n",
    "for divTag in divTags:\n",
    "    ### edge cases for Bath,Shoes,Watches\n",
    "    if divTag.find('h2').text == 'Bathroom Collections':\n",
    "        url_list.append('https://www.macys.com/shop/bed-bath/shower-accessories?id=8237&edge=hybrid')\n",
    "        continue\n",
    "    if divTag.find('h2').text == 'Shoes':\n",
    "        url_list.append('https://www.macys.com/shop/mens-clothing/shop-all-mens-footwear?id=55822&edge=hybrid')\n",
    "        continue\n",
    "    if divTag.find('h2').text == 'Watches':\n",
    "        url_list.append('https://www.macys.com/shop/jewelry-watches/womens-watches?id=57385')\n",
    "        continue\n",
    "    if divTag.find('h2').text == 'Juniors Clothing':\n",
    "        url_list.append('https://www.macys.com/shop/junior-clothing/shop-all-juniors-apparel?id=60983&edge=hybrid')\n",
    "        continue    \n",
    "    ### General Cases    \n",
    "    if divTag.find('h2').text==divTag.find('a', href=True).text:\n",
    "        a=divTag.find('a', href=True)\n",
    "        url_list.append(a['href'])\n",
    "    else:\n",
    "        sub_list = []\n",
    "        flag_all = True\n",
    "        for a in divTag.find_all('a', href=True):\n",
    "            sub_list.append(a['href'])\n",
    "            if a.text.find(\"All\")!=-1:\n",
    "                url_list.append(a['href'])\n",
    "                flag_all = False\n",
    "        if flag_all:\n",
    "            url_list+=sub_list\n",
    "                \n",
    "    if(divTag.h2.string=='Rugs'):\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: collect items in each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In each page, we try to find the nextpage where to search further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchnext(sp):\n",
    "    nextli = sp.find(\"li\",class_=\"nextPage\")\n",
    "    if nextli:\n",
    "        nextpage = nextli.find(\"a\",href=True)\n",
    "        if nextpage and nextpage['href']!='#':\n",
    "            root = \"http://www.macys.com\"\n",
    "            if nextpage['href'].find(\"macys.com\")==-1:\n",
    "                nextlink = root+nextpage['href']\n",
    "            else:\n",
    "                nextlink = nextpage['href']\n",
    "        else:\n",
    "            nextlink = None\n",
    "        return nextlink\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In each page, collect the product names inside the main div block\n",
    "\n",
    "* There are some recommendation grid and historical views which we want to ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prod_page(sp):\n",
    "    tmp_list=[]\n",
    "    main_div = sp.find(\"div\", class_=\"sortableGrid\")\n",
    "    if main_div:\n",
    "        for div in main_div.find_all(\"div\", class_=\"productDescription\"):\n",
    "            for a in div.find_all(\"a\", class_=\"productDescLink\", title=True):\n",
    "                tmp_list.append(a['title'].strip())\n",
    "    else:\n",
    "        main_div = sp.find(\"div\", id=\"macysGlobalLayout\")\n",
    "        if main_div:\n",
    "            for div in main_div.find_all(\"div\", class_=\"shortDescription\"):\n",
    "                for a in div.find_all(\"a\", class_=\"productThumbnailLink\", href=True):\n",
    "                    tmp_list.append(a.text.strip())\n",
    "        \n",
    "    return tmp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each category, we traverse to the end of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url=\"http://www.macys.com/shop/womens-clothing/dresses/Pageindex/95?id=5449\"\n",
    "def search_cat(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    sp =BeautifulSoup(response.text, 'html.parser') \n",
    "    cat_list = []\n",
    "    cat_list+=prod_page(sp)\n",
    "    nextlink = searchnext(sp)\n",
    "    #i=0\n",
    "    while(nextlink):\n",
    "        response = requests.get(nextlink, headers=headers)\n",
    "        sp =BeautifulSoup(response.text, 'html.parser') \n",
    "        cat_list+=prod_page(sp)\n",
    "        nextlink = searchnext(sp)\n",
    "    return cat_list\n",
    "    #i+=1\n",
    "    #print('{0}'.format(i),end='\\r')\n",
    "    #print('{0}'.format(nextlink),end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save product list for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cat(cat_list,i):\n",
    "    filename = \"products/product_list_macy\"+str(i)+\".txt\"\n",
    "    with open(filename, \"a\") as myfile:\n",
    "        myfile.write('\\n'.join(cat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\r"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for url in url_list:\n",
    "    cat_list=search_cat(url)\n",
    "    save_cat(cat_list,i)\n",
    "    print('{0}'.format(i),end ='\\r')\n",
    "    #if len(cat_list)<1:\n",
    "    #    print('{0} abnorm'.format(i))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some bad links:\n",
    "\n",
    "* http://www1.macys.com/shop/for-the-home/piggy-banks-snow-globes?id=60821\n",
    "* http://www1.macys.com/shop/kitchen/combination-coffee-machines?id=43081\n",
    "* http://www1.macys.com/shop/dining-entertaining/individual-bowls?id=61072\n",
    "* http://www1.macys.com/shop/bed-bath/apartment-bedding?id=60167\n",
    "* http://www1.macys.com/shop/makeup-and-perfume/false-eyelashes?id=59291\n",
    "\n",
    "Wedding and Plus-size are the main directories, assume the subdirectories will cover them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 aggregate and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'products/'\n",
    "frame=[]\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.txt'):\n",
    "        df=pd.read_csv(path+file,names=['product'],sep='\\t')\n",
    "        frame.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = pd.concat(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195311"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There could be redundant product as some repeated search, perform a simple clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prod_list = res['product'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112744"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prod_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('products/prodcuts_final.csv', \"w\") as myfile:\n",
    "        myfile.write('\\n'.join(prod_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('products/prodcuts_full.csv',index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genearlize\n",
    "the framework can be summarized and genearilzed as:\n",
    "* start from site map, if a valid sitemap.xml file could be obtained that would be perfect, otherwise perform the parsing of sitemap webpage\n",
    " \n",
    "* search each merchandize category from the root to the end, in this step, each website may need to modify the parsing rules, however the framework could be the same\n",
    "\n",
    "* perform cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment\n",
    "\n",
    "some simple check:\n",
    "* (count:496) http://www1.macys.com/shop/makeup-and-perfume/makeup-brushes-and-makeup-bags?id=56285\n",
    "* (count:41)http://www1.macys.com/shop/plus-size-clothing/shop-jumpsuits-rompers?id=43910\n",
    "* (count:43)http://www1.macys.com/shop/kitchen/toasters-toaster-ovens?id=7575\n",
    "* (count:7)http://www1.macys.com/cms/slp/2/Womens-Board-Shorts\n",
    "* (count:40)http://www1.macys.com/shop/kitchen/grills-griddles?id=7569\n",
    "\n",
    "All above checks are correct, shows below.\n",
    "\n",
    "However, we assume that from we can traverse all the products by using the categories in the sitemap\n",
    "\n",
    "It is possible that some items which may not be reached as we don't know how good is the sitemap\n",
    "\n",
    "A dfs or other traversing algorithm may be needed to go through every possible single item page for validation\n",
    "\n",
    "However, that may takes too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 56 225 19 231 "
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "for i in range(5):\n",
    "    print(randint(0, len(url_list)),end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496 41 43 7 40 "
     ]
    }
   ],
   "source": [
    "for i in [145,56,225,19,231]:\n",
    "    file = 'products/product_list_macy'+str(i)+'.txt'\n",
    "    df=pd.read_csv(file,names=['product'],sep='\\t')\n",
    "    print(df.shape[0],end=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
